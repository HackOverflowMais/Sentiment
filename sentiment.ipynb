{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_spPLvH2kzxP",
        "outputId": "23a9641d-f366-453a-d3bd-ad195d70e782"
      },
      "source": [
        "!pip install keras==2.2.4\r\n",
        "!pip install -q pydot\r\n",
        "!pip install graphviz\r\n",
        "!apt-get install graphviz\r\n",
        "from keras.utils.vis_utils import plot_model\r\n",
        "!pip install git+https://www.github.com/keras-team/keras-contrib.git\r\n",
        "!pip install tensorflow==1.15.0\r\n",
        "import os\r\n",
        "from numpy import array\r\n",
        "from keras.preprocessing.text import one_hot\r\n",
        "from keras.preprocessing.sequence import pad_sequences\r\n",
        "from keras.models import Sequential\r\n",
        "from keras.layers.core import Activation, Dropout, Dense\r\n",
        "from keras.layers import Flatten, LSTM\r\n",
        "from keras.layers import GlobalMaxPooling1D\r\n",
        "from keras.models import Model\r\n",
        "from keras.layers.embeddings import Embedding\r\n",
        "from sklearn.model_selection import train_test_split\r\n",
        "from keras.preprocessing.text import Tokenizer\r\n",
        "from keras.layers import Input\r\n",
        "from keras.layers.merge import Concatenate\r\n",
        "from gensim.models import Word2Vec\r\n",
        "from gensim import models\r\n",
        "from gensim.models.keyedvectors import KeyedVectors\r\n",
        "from sklearn.model_selection import train_test_split\r\n",
        "import pandas as pd\r\n",
        "import numpy as np\r\n",
        "import re\r\n",
        "import csv\r\n",
        "import pandas as pd\r\n",
        "import numpy as np\r\n",
        "import re\r\n",
        "\r\n",
        "import seaborn as sns\r\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: keras==2.2.4 in /usr/local/lib/python3.6/dist-packages (2.2.4)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras==2.2.4) (2.10.0)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from keras==2.2.4) (1.4.1)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from keras==2.2.4) (1.0.8)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from keras==2.2.4) (1.15.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from keras==2.2.4) (1.1.2)\n",
            "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.6/dist-packages (from keras==2.2.4) (1.19.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from keras==2.2.4) (3.13)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M3tr7MTjmg4P"
      },
      "source": [
        "from tqdm import tqdm, trange\r\n",
        "from google.colab import files\r\n",
        "uploaded = files.upload()\r\n",
        "\r\n",
        "data_raw = pd.read_csv(\"data.csv\")\r\n",
        "\r\n",
        "print(\"Number of rows in data =\",data_raw.shape[0])\r\n",
        "print(\"Number of columns in data =\",data_raw.shape[1])\r\n",
        "print(\"\\n\")\r\n",
        "print(\"**Sample data:**\")\r\n",
        "\r\n",
        "data_raw.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5ELzhA6MzJ-1"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JeSL_3LmmhAh"
      },
      "source": [
        "#One hot encoding for all labels\r\n",
        "data_raw\r\n",
        "j=0\r\n",
        "for i in data_raw.iterrows():\r\n",
        "    if data_raw.worry[j]>=5:\r\n",
        "        data_raw.worry[j]=1\r\n",
        "    else:\r\n",
        "        data_raw.worry[j]=0\r\n",
        "    j=j+1\r\n",
        "j=0\r\n",
        "for i in data_raw.iterrows():\r\n",
        "    if data_raw.anger[j]>=5:\r\n",
        "        data_raw.anger[j]=1\r\n",
        "    else:\r\n",
        "        data_raw.anger[j]=0\r\n",
        "    j=j+1\r\n",
        "j=0\r\n",
        "for i in data_raw.iterrows():\r\n",
        "    if data_raw.anxiety[j]>=5:\r\n",
        "        data_raw.anxiety[j]=1\r\n",
        "    else:\r\n",
        "        data_raw.anxiety[j]=0\r\n",
        "    j=j+1\r\n",
        "j=0\r\n",
        "for i in data_raw.iterrows():\r\n",
        "    if data_raw.sadness[j]>=5:\r\n",
        "        data_raw.sadness[j]=1\r\n",
        "    else:\r\n",
        "        data_raw.sadness[j]=0\r\n",
        "    j=j+1\r\n",
        "j=0\r\n",
        "for i in data_raw.iterrows():\r\n",
        "    if data_raw.desire[j]>=5:\r\n",
        "        data_raw.desire[j]=1\r\n",
        "    else:\r\n",
        "        data_raw.desire[j]=0\r\n",
        "    j=j+1\r\n",
        "\r\n",
        "j=0\r\n",
        "for i in data_raw.iterrows():\r\n",
        "    if data_raw.disgust[j]>=5:\r\n",
        "        data_raw.disgust[j]=1\r\n",
        "    else:\r\n",
        "        data_raw.disgust[j]=0\r\n",
        "    j=j+1\r\n",
        "j=0\r\n",
        "for i in data_raw.iterrows():\r\n",
        "    if data_raw.fear[j]>=5:\r\n",
        "        data_raw.fear[j]=1\r\n",
        "    else:\r\n",
        "        data_raw.fear[j]=0\r\n",
        "    j=j+1\r\n",
        "j=0\r\n",
        "for i in data_raw.iterrows():\r\n",
        "    if data_raw.happiness[j]>=5:\r\n",
        "        data_raw.happiness[j]=1\r\n",
        "    else:\r\n",
        "        data_raw.happiness[j]=0\r\n",
        "    j=j+1\r\n",
        "j=0\r\n",
        "for i in data_raw.iterrows():\r\n",
        "    if data_raw.relaxation[j]>=5:\r\n",
        "        data_raw.relaxation[j]=1\r\n",
        "    else:\r\n",
        "        data_raw.relaxation[j]=0\r\n",
        "    j=j+1\r\n",
        "\r\n",
        "\r\n",
        "        \r\n",
        "\r\n",
        "data_raw\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "crGoCtr-mhC5"
      },
      "source": [
        "\r\n",
        "MAX_SEQUENCE_LENGTH=50\r\n",
        "data_raw.shape\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZTDy_KKGu_nT"
      },
      "source": [
        "emotion_labels=data_raw[['worry','anger','disgust','fear','anxiety','sadness','happiness','relaxation','desire']]\r\n",
        "fig_size = plt.rcParams[\"figure.figsize\"]\r\n",
        "fig_size[0] = 10\r\n",
        "fig_size[1] = 8\r\n",
        "plt.rcParams[\"figure.figsize\"] = fig_size\r\n",
        "plt.xlabel('Emotions')\r\n",
        "plt.ylabel('Total number of tweets')\r\n",
        "plt.title('Distribution of tweets in India at a glance')\r\n",
        "\r\n",
        "emotion_labels.sum(axis=0).plot.bar()\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WsYquL74vDIi"
      },
      "source": [
        "import tensorflow as tf\r\n",
        "print(tf.__version__)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uzHCyTGGzQx9"
      },
      "source": [
        "def preprocess_text(sen): \r\n",
        "    # Remove punctuations and numbers\r\n",
        "    sentence = re.sub('[^a-zA-Z]', ' ', sen)\r\n",
        "\r\n",
        "    # Single character removal\r\n",
        "    sentence = re.sub(r\"\\s+[a-zA-Z]\\s+\", ' ', sentence)\r\n",
        "\r\n",
        "    # Removing multiple spaces\r\n",
        "    sentence = re.sub(r'\\s+', ' ', sentence)\r\n",
        "\r\n",
        "    return sentence\r\n",
        "data_raw['Text_Clean'] = data_raw['text_long'].apply(lambda x: preprocess_text(x))\r\n",
        "\r\n",
        "\r\n",
        "import nltk\r\n",
        "nltk.download('punkt')\r\n",
        "nltk.download('stopwords')\r\n",
        "from nltk import word_tokenize, WordNetLemmatizer\r\n",
        "\r\n",
        "\r\n",
        "tokens = [word_tokenize(sen) for sen in data_raw.Text_Clean]\r\n",
        "def lower_token(tokens): \r\n",
        "    return [w.lower() for w in tokens]    \r\n",
        "    \r\n",
        "lower_tokens = [lower_token(token) for token in tokens] \r\n",
        "\r\n",
        "\r\n",
        "from nltk.corpus import stopwords\r\n",
        "stoplist = stopwords.words('english')\r\n",
        "\r\n",
        "def remove_stop_words(tokens):\r\n",
        "    return [word for word in tokens if word not in stoplist]\r\n",
        "\r\n",
        "filtered_words = [remove_stop_words(sen) for sen in lower_tokens] \r\n",
        "result = [' '.join(sen) for sen in filtered_words]\r\n",
        "data_raw['Text_Final'] = result\r\n",
        "data_raw['tokens'] = filtered_words\r\n",
        "data_raw=data_raw[['Text_Final', 'tokens','worry','anger','disgust','fear','anxiety','sadness','happiness','relaxation','desire']]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OkNZ7haXzQ0z"
      },
      "source": [
        "data_raw"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eeaji-p_zQ23"
      },
      "source": [
        "!pip install -U sentence-transformers\r\n",
        "sentences= data_raw.Text_Final.values.tolist()\r\n",
        "from sentence_transformers import SentenceTransformer\r\n",
        "vec = SentenceTransformer('bert-base-nli-mean-tokens')\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MGngLczlzQ6B"
      },
      "source": [
        "data_train, data_test = train_test_split(data_raw, test_size=0.10, random_state=42)\r\n",
        "data_raw['Text_Final']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nq8CN_uNzQ8-"
      },
      "source": [
        "all_training_words = [word for tokens in data_train[\"tokens\"] for word in tokens]\r\n",
        "training_sentence_lengths = [len(tokens) for tokens in data_train[\"tokens\"]]\r\n",
        "TRAINING_VOCAB = sorted(list(set(all_training_words)))\r\n",
        "print(\"%s words total, with a vocabulary size of %s\" % (len(all_training_words), len(TRAINING_VOCAB)))\r\n",
        "print(\"Max sentence length is %s\" % max(training_sentence_lengths))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tDVFq-DDzQ_n"
      },
      "source": [
        "all_test_words = [word for tokens in data_test[\"tokens\"] for word in tokens]\r\n",
        "test_sentence_lengths = [len(tokens) for tokens in data_test[\"tokens\"]]\r\n",
        "TEST_VOCAB = sorted(list(set(all_test_words)))\r\n",
        "print(\"%s words total, with a vocabulary size of %s\" % (len(all_test_words), len(TEST_VOCAB)))\r\n",
        "print(\"Max sentence length is %s\" % max(test_sentence_lengths))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v7xnCEtmzP6c"
      },
      "source": [
        "MAX_SEQUENCE_LENGTH = 50\r\n",
        "EMBEDDING_DIM = 768"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wrb6lSwJ1YRH"
      },
      "source": [
        "tokenizer = Tokenizer(num_words=len(TRAINING_VOCAB), lower=True, char_level=False)\r\n",
        "tokenizer.fit_on_texts(data_train[\"Text_Final\"].tolist())\r\n",
        "training_sequences = tokenizer.texts_to_sequences(data_train[\"Text_Final\"].tolist())\r\n",
        "\r\n",
        "train_word_index = tokenizer.word_index\r\n",
        "print('Found %s unique tokens.' % len(train_word_index))\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uwub5thh1Zi5"
      },
      "source": [
        "num_words = len(train_word_index)\r\n",
        "train_rnn_data = pad_sequences(training_sequences, maxlen=MAX_SEQUENCE_LENGTH)\r\n",
        "len(training_sequences)\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x5ZRxQvW1rVA"
      },
      "source": [
        "train_word_index"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0OyGiAd31xAk"
      },
      "source": [
        "train_embedding_weights = np.zeros((len(train_word_index)+1, EMBEDDING_DIM))\r\n",
        "for word,index in train_word_index.items():\r\n",
        "    \r\n",
        "    train_embedding_weights[index,:] = vec.encode(word)\r\n",
        "print(train_embedding_weights.shape)\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cfOZRuJL2Wcl"
      },
      "source": [
        "test_sequences = tokenizer.texts_to_sequences(data_test[\"Text_Final\"].tolist())\r\n",
        "test_rnn_data = pad_sequences(test_sequences, maxlen=MAX_SEQUENCE_LENGTH)\r\n",
        "data_test.shape"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}